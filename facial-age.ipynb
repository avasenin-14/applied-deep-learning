{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. Simple age estimator"
      ],
      "metadata": {
        "id": "YSwS9FRVVai4"
      },
      "id": "YSwS9FRVVai4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading and preparation"
      ],
      "metadata": {
        "id": "6LiFiFHOhqqD"
      },
      "id": "6LiFiFHOhqqD"
    },
    {
      "cell_type": "markdown",
      "id": "efc27446",
      "metadata": {
        "id": "efc27446"
      },
      "source": [
        "Import dataset from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cf7cda7",
      "metadata": {
        "id": "6cf7cda7"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"frabbisw/facial-age\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "models_dir = '/content/drive/MyDrive/models'"
      ],
      "metadata": {
        "id": "CfKXbrPMbZRL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CfKXbrPMbZRL"
    },
    {
      "cell_type": "markdown",
      "id": "639ad29b",
      "metadata": {
        "id": "639ad29b"
      },
      "source": [
        "Check the structure of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e36c46b3",
      "metadata": {
        "id": "e36c46b3"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "path = Path(path)\n",
        "\n",
        "def hierarchy(root: Path):\n",
        "    return { child: hierarchy(child) for child in root.iterdir() } if root.is_dir() else None\n",
        "\n",
        "path_dict = hierarchy(path)\n",
        "path_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d513da6",
      "metadata": {
        "id": "7d513da6"
      },
      "outputs": [],
      "source": [
        "path_dict = dict(list(path_dict.items())[0:])\n",
        "\n",
        "path_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de1e9e3d",
      "metadata": {
        "id": "de1e9e3d"
      },
      "outputs": [],
      "source": [
        "data = {}\n",
        "\n",
        "for directory in path_dict:\n",
        "    for age in path_dict[directory]:\n",
        "        for image in path_dict[directory][age]:\n",
        "            if image.is_file():\n",
        "                data[image] = age.name\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e171802",
      "metadata": {
        "id": "0e171802"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data = {'file' : data.keys(), 'age' : data.values()})\n",
        "print(df.sample(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ee7e3c7",
      "metadata": {
        "id": "6ee7e3c7"
      },
      "source": [
        "After manually inspecting the dataset, it was decided to drop some of the examples as they were either corrupted files, different body parts than face or having the wrong age."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97bef9e4",
      "metadata": {
        "id": "97bef9e4"
      },
      "outputs": [],
      "source": [
        "print(len(df))\n",
        "files_to_drop = [3829, 4313, 7034, 7326, 9378, 1490,]\n",
        "for filename in files_to_drop:\n",
        "    filename_with_extension = f\"{filename}.png\"\n",
        "    for index, row in df.iterrows():\n",
        "        if filename_with_extension in str(row.file):\n",
        "            print(row.file)\n",
        "            df = df.drop(index)\n",
        "\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9644ff87",
      "metadata": {
        "id": "9644ff87"
      },
      "outputs": [],
      "source": [
        "df.to_csv('facial-age.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145a7395",
      "metadata": {
        "id": "145a7395"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "02239e40",
      "metadata": {
        "id": "02239e40"
      },
      "source": [
        "We can categorize ages into bins to simplify classification task.\n",
        "To start with it we can simple create bins with approximately equal number of examples in each.\n",
        "The downside of it is that we won't have as precise age in case of wider bins (where number of examples were low for some ages). And the upside is that we will have approximarely equal number of examples in each bin, which in theory will allow us to predict a category with more accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1098c7b2",
      "metadata": {
        "id": "1098c7b2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.countplot(data=df['age'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbea41f1",
      "metadata": {
        "id": "bbea41f1"
      },
      "outputs": [],
      "source": [
        "df['age'] = df['age'].astype(int)\n",
        "df['age_bins'] = pd.qcut(x=df['age'], q=8, precision=0)\n",
        "print(df.sample(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a99dd32e",
      "metadata": {
        "id": "a99dd32e"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=df['age_bins'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['age_bins'] = df['age_bins'].astype(str)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "-usOXoBL2Qyz"
      },
      "id": "-usOXoBL2Qyz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "train, validate, test = \\\n",
        "              np.split(df.sample(frac=1),\n",
        "                       [int(.75*len(df)), int(.9*len(df))])\n",
        "\n",
        "print(len(train), len(validate), len(test))"
      ],
      "metadata": {
        "id": "vBV_lVf81yMq"
      },
      "id": "vBV_lVf81yMq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in (train, validate, test):\n",
        "  sns.countplot(data=i['age_bins'])"
      ],
      "metadata": {
        "id": "ZEhPsJBRuiqh"
      },
      "id": "ZEhPsJBRuiqh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The proportions might be not exactly equal, but we can attribute it to some randomness in sampling."
      ],
      "metadata": {
        "id": "XLyf5ZfQx0Vb"
      },
      "id": "XLyf5ZfQx0Vb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giving up and starting using LLMs to have a chance to get things done before the deadline."
      ],
      "metadata": {
        "id": "VYe_Vx2gTAod"
      },
      "id": "VYe_Vx2gTAod"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimenting with CNNs"
      ],
      "metadata": {
        "id": "6OUAmpq3hxrV"
      },
      "id": "6OUAmpq3hxrV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I was confused how CNN will work with images considering I only have filepaths for them. Turns out we need (or may) use Image Data Generators for this task."
      ],
      "metadata": {
        "id": "Eis42dDWh2Xe"
      },
      "id": "Eis42dDWh2Xe"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Ensure columns are strings\n",
        "train['file'] = train['file'].astype(str)\n",
        "train['age_bins'] = train['age_bins'].astype(str)\n",
        "validate['file'] = validate['file'].astype(str)\n",
        "validate['age_bins'] = validate['age_bins'].astype(str)\n",
        "test['file'] = test['file'].astype(str)\n",
        "test['age_bins'] = test['age_bins'].astype(str)\n",
        "\n",
        "# Create ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Setup generators\n",
        "# class_mode='sparse' produces integer labels, matching 'sparse_categorical_crossentropy'\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=train,\n",
        "    x_col='file',\n",
        "    y_col='age_bins',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=validate,\n",
        "    x_col='file',\n",
        "    y_col='age_bins',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "test_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=test,\n",
        "    x_col='file',\n",
        "    y_col='age_bins',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse'\n",
        ")"
      ],
      "metadata": {
        "id": "ZBqvbJD0x_Ny"
      },
      "id": "ZBqvbJD0x_Ny",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "ZkM-GZVE4KXA"
      },
      "id": "ZkM-GZVE4KXA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Returning to manually writing code because gemini is too slow today to be useful. Returning back to Gemini to ask how to work with image data instead of path strings."
      ],
      "metadata": {
        "id": "R7hL1uGh2GY1"
      },
      "id": "R7hL1uGh2GY1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Haha, that is somewhat low accuracy. Let's try other activations, and then maybe different number of epochs, and then if it won't help maybe read something on different combinations of layers."
      ],
      "metadata": {
        "id": "Tz5bYQzU7nIV"
      },
      "id": "Tz5bYQzU7nIV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I could say that it fits in my world map after the lecture, that tanh will converge faster. However, what I don't understand is why the starting accuracy way higher with this model. Is it just random luck or if there something else I don't understand?"
      ],
      "metadata": {
        "id": "RG0fSPdTAMfq"
      },
      "id": "RG0fSPdTAMfq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ah, so training accuracy is way higher than validation accuracy. I wonder how can I prevent overfitting here.. :)"
      ],
      "metadata": {
        "id": "N_bj6YPLBKqk"
      },
      "id": "N_bj6YPLBKqk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now combining three of them in one cycle to avoid copy-pasted code."
      ],
      "metadata": {
        "id": "GAiD5VOIG1qn"
      },
      "id": "GAiD5VOIG1qn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different hidden layers activation functions"
      ],
      "metadata": {
        "id": "a9LtGomdiIE0"
      },
      "id": "a9LtGomdiIE0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eb47c7d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "activations = ['sigmoid', 'tanh', 'relu']\n",
        "results = {}\n",
        "\n",
        "for activation in activations:\n",
        "    print(f\"\\n{'='*60}\\nProcessing model with {activation} activation\\n{'='*60}\")\n",
        "\n",
        "    # 1. Define Model\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation=activation, input_shape=(64, 64, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation=activation),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation=activation),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation=activation),\n",
        "        layers.Dense(8, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 2. Load or Train\n",
        "    model_filename = models_dir + f'model_{activation}.keras'\n",
        "    history_filename = models_dir + f'history_{activation}.pkl'\n",
        "\n",
        "    if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "        print(f\"Loading existing model and history for {activation}...\")\n",
        "        model = keras.models.load_model(model_filename)\n",
        "        with open(history_filename, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "    else:\n",
        "        print(f\"Training model with {activation}...\")\n",
        "        history_obj = model.fit(\n",
        "            train_generator,\n",
        "            epochs=10,\n",
        "            validation_data=validation_generator,\n",
        "            verbose=1\n",
        "        )\n",
        "        history = history_obj.history\n",
        "\n",
        "        # Save model and history\n",
        "        model.save(model_filename)\n",
        "        with open(history_filename, 'wb') as f:\n",
        "            pickle.dump(history, f)\n",
        "        print(f\"Saved model and history for {activation}.\")\n",
        "\n",
        "    # 3. Evaluate on Test Data\n",
        "    print(f\"Evaluating {activation} model on test data...\")\n",
        "    test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    results[activation] = {'test_acc': test_acc, 'test_loss': test_loss}\n",
        "\n",
        "    # 4. Plotting (3 Graphs: Accuracy, Loss, Val vs Test)\n",
        "    acc = history['accuracy']\n",
        "    val_acc = history['val_accuracy']\n",
        "    loss = history['loss']\n",
        "    val_loss = history['val_loss']\n",
        "    epochs_range = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    # -- Graph 1: Accuracy --\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs_range, acc, 'bo-', label='Training acc')\n",
        "    plt.plot(epochs_range, val_acc, 'b', label='Validation acc')\n",
        "    plt.title(f'{activation.capitalize()} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "\n",
        "    # -- Graph 2: Loss --\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs_range, loss, 'bo-', label='Training loss')\n",
        "    plt.plot(epochs_range, val_loss, 'b', label='Validation loss')\n",
        "    plt.title(f'{activation.capitalize()} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # -- Graph 3: Val vs Test Comparison --\n",
        "    plt.subplot(1, 3, 3)\n",
        "    metrics_names = ['Accuracy', 'Loss']\n",
        "    val_values = [val_acc[-1], val_loss[-1]]\n",
        "    test_values = [test_acc, test_loss]\n",
        "\n",
        "    x_pos = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "\n",
        "    rects1 = plt.bar(x_pos - width/2, val_values, width, label='Validation')\n",
        "    rects2 = plt.bar(x_pos + width/2, test_values, width, label='Test')\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(f'{activation.capitalize()} - Val vs Test')\n",
        "    plt.xticks(x_pos, metrics_names)\n",
        "    plt.legend()\n",
        "\n",
        "    # Add labels\n",
        "    plt.bar_label(rects1, padding=3, fmt='%.2f')\n",
        "    plt.bar_label(rects2, padding=3, fmt='%.2f')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "4eb47c7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt: What are different kernel sizes that can be used in CNNs?"
      ],
      "metadata": {
        "id": "i8P3xFJ2IuEN"
      },
      "id": "i8P3xFJ2IuEN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e4c33d1"
      },
      "source": [
        "Responce:\n",
        "### Kernel Sizes in Convolutional Neural Networks (CNNs)\n",
        "\n",
        "The **kernel size** (or filter size) determines the dimensions of the sliding window that scans over the input image. Here are the most common sizes and their characteristics:\n",
        "\n",
        "1.  **3x3 Kernels ( The Standard)**:\n",
        "    *   **Usage**: By far the most popular choice (used in architectures like VGG).\n",
        "    *   **Why**: It captures local features effectively with a small number of parameters. Stacking multiple 3x3 layers allows the network to learn complex patterns with a large receptive field but fewer parameters than a single large kernel.\n",
        "\n",
        "2.  **5x5 and 7x7 Kernels**:\n",
        "    *   **Usage**: Often used in the very first layer of a network.\n",
        "    *   **Why**: These are useful for capturing larger, more general features (like basic shapes or blobs) directly from the raw image. However, they are computationally more expensive.\n",
        "\n",
        "3.  **1x1 Kernels (Pointwise Convolution)**:\n",
        "    *   **Usage**: Popularized by the Inception network.\n",
        "    *   **Why**: They don't look at neighboring pixels. Instead, they are used to **change the number of channels** (depth) of the feature map (dimensionality reduction) or to add non-linearity without changing the spatial dimensions.\n",
        "\n",
        "4.  **Why Odd Numbers?** (3x3, 5x5, etc.)\n",
        "    *   Kernels are almost always odd-sized (e.g., 3, 5, 7) so that they have a **center pixel**. This makes it easier to align the filter with the input pixels and apply padding symmetrically."
      ],
      "id": "9e4c33d1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good point about odd numbers, I didn' know that. And got a bit better understanding of kernel sizes overall."
      ],
      "metadata": {
        "id": "kFnmrJWK4DIK"
      },
      "id": "kFnmrJWK4DIK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimenting with different kernel sizes for convolution layers."
      ],
      "metadata": {
        "id": "GbkN6rBTie6Y"
      },
      "id": "GbkN6rBTie6Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be4f131c"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "kernels = [(3, 3), (5, 5), (7, 7)]\n",
        "results_kernels = {}\n",
        "\n",
        "for kernel_size in kernels:\n",
        "    print(f\"\\n{'='*60}\\nProcessing model with kernel size: {kernel_size[0]}x{kernel_size[1]}\\n{'='*60}\")\n",
        "\n",
        "    # 1. Define Model\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, kernel_size, activation='relu', input_shape=(64, 64, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, kernel_size, activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, kernel_size, activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(8, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 2. Load or Train\n",
        "    model_filename = models_dir + f'model_kernel_{kernel_size[0]}x{kernel_size[1]}.keras'\n",
        "    history_filename = models_dir + f'history_kernel_{kernel_size[0]}x{kernel_size[1]}.pkl'\n",
        "\n",
        "    if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "        print(f\"Loading existing model and history for kernel {kernel_size[0]}x{kernel_size[1]}...\")\n",
        "        model = keras.models.load_model(model_filename)\n",
        "        with open(history_filename, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "    else:\n",
        "        print(f\"Training model with kernel {kernel_size[0]}x{kernel_size[1]}...\")\n",
        "        history_obj = model.fit(\n",
        "            train_generator,\n",
        "            epochs=10,\n",
        "            validation_data=validation_generator,\n",
        "            verbose=1\n",
        "        )\n",
        "        history = history_obj.history\n",
        "\n",
        "        # Save model and history\n",
        "        model.save(model_filename)\n",
        "        with open(history_filename, 'wb') as f:\n",
        "            pickle.dump(history, f)\n",
        "        print(f\"Saved model and history for kernel {kernel_size[0]}x{kernel_size[1]}.\")\n",
        "\n",
        "    # 3. Evaluate on Test Data\n",
        "    print(f\"Evaluating kernel {kernel_size[0]}x{kernel_size[1]} model on test data...\")\n",
        "    test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    results_kernels[f'{kernel_size[0]}x{kernel_size[1]}'] = {'test_acc': test_acc, 'test_loss': test_loss}\n",
        "\n",
        "    # 4. Plotting (3 Graphs: Accuracy, Loss, Val vs Test)\n",
        "    acc = history['accuracy']\n",
        "    val_acc = history['val_accuracy']\n",
        "    loss = history['loss']\n",
        "    val_loss = history['val_loss']\n",
        "    epochs_range = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    # -- Graph 1: Accuracy --\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs_range, acc, 'bo-', label='Training acc')\n",
        "    plt.plot(epochs_range, val_acc, 'b', label='Validation acc')\n",
        "    plt.title(f'Kernel {kernel_size[0]}x{kernel_size[1]} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "\n",
        "    # -- Graph 2: Loss --\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs_range, loss, 'bo-', label='Training loss')\n",
        "    plt.plot(epochs_range, val_loss, 'b', label='Validation loss')\n",
        "    plt.title(f'Kernel {kernel_size[0]}x{kernel_size[1]} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # -- Graph 3: Val vs Test Comparison --\n",
        "    plt.subplot(1, 3, 3)\n",
        "    metrics_names = ['Accuracy', 'Loss']\n",
        "    val_values = [val_acc[-1], val_loss[-1]]\n",
        "    test_values = [test_acc, test_loss]\n",
        "\n",
        "    x_pos = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "\n",
        "    rects1 = plt.bar(x_pos - width/2, val_values, width, label='Validation')\n",
        "    rects2 = plt.bar(x_pos + width/2, test_values, width, label='Test')\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(f'Kernel {kernel_size[0]}x{kernel_size[1]} - Val vs Test')\n",
        "    plt.xticks(x_pos, metrics_names)\n",
        "    plt.legend()\n",
        "\n",
        "    # Add labels\n",
        "    plt.bar_label(rects1, padding=3, fmt='%.2f')\n",
        "    plt.bar_label(rects2, padding=3, fmt='%.2f')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "be4f131c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like smaller features allow a bit faster convergence. Though I'm not sure it will always be the case, and in theory for some data larger kernels might catch features that would discriminate more."
      ],
      "metadata": {
        "id": "7grrTsOUaCio"
      },
      "id": "7grrTsOUaCio"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimenting with different model depths"
      ],
      "metadata": {
        "id": "ajvftGegiru9"
      },
      "id": "ajvftGegiru9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "948b2d6e"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "results_depth = {}\n",
        "model_depths = [1, 2, 3] # Representing shallow, medium, deep architectures\n",
        "\n",
        "for depth in model_depths:\n",
        "    print(f\"\\n{'='*60}\\nProcessing model with {depth} convolutional blocks\\n{'='*60}\")\n",
        "\n",
        "    # 1. Define Model based on depth\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    if depth >= 2:\n",
        "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "        model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    if depth >= 3:\n",
        "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "        model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(8, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 2. Load or Train\n",
        "    model_filename = os.path.join(models_dir, f'model_depth_{depth}.keras')\n",
        "    history_filename = os.path.join(models_dir, f'history_depth_{depth}.pkl')\n",
        "\n",
        "    if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "        print(f\"Loading existing model and history for depth {depth}...\")\n",
        "        model = keras.models.load_model(model_filename)\n",
        "        with open(history_filename, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "    else:\n",
        "        print(f\"Training model with depth {depth}...\")\n",
        "        history_obj = model.fit(\n",
        "            train_generator,\n",
        "            epochs=10,\n",
        "            validation_data=validation_generator,\n",
        "            verbose=1\n",
        "        )\n",
        "        history = history_obj.history\n",
        "\n",
        "        # Save model and history\n",
        "        model.save(model_filename)\n",
        "        with open(history_filename, 'wb') as f:\n",
        "            pickle.dump(history, f)\n",
        "        print(f\"Saved model and history for depth {depth}.\")\n",
        "\n",
        "    # 3. Evaluate on Test Data\n",
        "    print(f\"Evaluating depth {depth} model on test data...\")\n",
        "    test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    results_depth[depth] = {'test_acc': test_acc, 'test_loss': test_loss}\n",
        "\n",
        "    # 4. Plotting (3 Graphs: Accuracy, Loss, Val vs Test)\n",
        "    acc = history['accuracy']\n",
        "    val_acc = history['val_accuracy']\n",
        "    loss = history['loss']\n",
        "    val_loss = history['val_loss']\n",
        "    epochs_range = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    # -- Graph 1: Accuracy --\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs_range, acc, 'bo-', label='Training acc')\n",
        "    plt.plot(epochs_range, val_acc, 'b', label='Validation acc')\n",
        "    plt.title(f'Depth {depth} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "\n",
        "    # -- Graph 2: Loss --\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs_range, loss, 'bo-', label='Training loss')\n",
        "    plt.plot(epochs_range, val_loss, 'b', label='Validation loss')\n",
        "    plt.title(f'Depth {depth} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # -- Graph 3: Val vs Test Comparison --\n",
        "    plt.subplot(1, 3, 3)\n",
        "    metrics_names = ['Accuracy', 'Loss']\n",
        "    val_values = [val_acc[-1], val_loss[-1]]\n",
        "    test_values = [test_acc, test_loss]\n",
        "\n",
        "    x_pos = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "\n",
        "    rects1 = plt.bar(x_pos - width/2, val_values, width, label='Validation')\n",
        "    rects2 = plt.bar(x_pos + width/2, test_values, width, label='Test')\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(f'Depth {depth} - Val vs Test')\n",
        "    plt.xticks(x_pos, metrics_names)\n",
        "    plt.legend()\n",
        "\n",
        "    # Add labels\n",
        "    plt.bar_label(rects1, padding=3, fmt='%.2f')\n",
        "    plt.bar_label(rects2, padding=3, fmt='%.2f')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "948b2d6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimenting with pooling"
      ],
      "metadata": {
        "id": "H-t03rSri5Pd"
      },
      "id": "H-t03rSri5Pd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35379432"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "results_pooling_comparison = {}\n",
        "\n",
        "# Define the 'pooling' CNN model\n",
        "model_pooling = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "model_pooling.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Define the 'no_pooling' CNN model\n",
        "model_no_pooling = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "model_no_pooling.compile(optimizer='adam',\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "models_to_train = {\n",
        "    'pooling': model_pooling,\n",
        "    'no_pooling': model_no_pooling\n",
        "}\n",
        "\n",
        "for model_name_key, model_obj in models_to_train.items():\n",
        "    print(f\"\\n{'='*60}\\nProcessing {model_name_key} model\\n{'='*60}\")\n",
        "\n",
        "    model_filename = os.path.join(models_dir, f'model_{model_name_key}.keras')\n",
        "    history_filename = os.path.join(models_dir, f'history_{model_name_key}.pkl')\n",
        "\n",
        "    if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "        print(f\"Loading existing model and history for {model_name_key}...\")\n",
        "        model = keras.models.load_model(model_filename)\n",
        "        with open(history_filename, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "    else:\n",
        "        print(f\"Training {model_name_key} model...\")\n",
        "        history_obj = model_obj.fit(\n",
        "            train_generator,\n",
        "            epochs=10,\n",
        "            validation_data=validation_generator,\n",
        "            verbose=1\n",
        "        )\n",
        "        history = history_obj.history\n",
        "\n",
        "        # Save model and history\n",
        "        model_obj.save(model_filename)\n",
        "        with open(history_filename, 'wb') as f:\n",
        "            pickle.dump(history, f)\n",
        "        print(f\"Saved model and history for {model_name_key}.\")\n",
        "\n",
        "    # Store history for later plotting\n",
        "    results_pooling_comparison[model_name_key] = history\n",
        "\n",
        "print(\"Models defined, trained (or loaded), and histories saved/stored.\")"
      ],
      "id": "35379432",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99788618"
      },
      "source": [
        "test_results = {}\n",
        "\n",
        "# Construct the full path to the saved 'pooling' model file\n",
        "model_filename_pooling = os.path.join(models_dir, 'model_pooling.keras')\n",
        "\n",
        "# Load the 'pooling' model\n",
        "model_pooling = keras.models.load_model(model_filename_pooling)\n",
        "\n",
        "# Evaluate the loaded 'pooling' model on the test_generator\n",
        "test_loss_pooling, test_acc_pooling = model_pooling.evaluate(test_generator, verbose=0)\n",
        "\n",
        "# Store the obtained test accuracy and test loss in the test_results dictionary\n",
        "test_results['pooling'] = {'test_acc': test_acc_pooling, 'test_loss': test_loss_pooling}\n",
        "\n",
        "print(f\"Pooling Model - Test Accuracy: {test_acc_pooling:.4f}\")\n",
        "print(f\"Pooling Model - Test Loss: {test_loss_pooling:.4f}\")\n",
        "print(\"Test results for 'pooling' model stored.\")"
      ],
      "id": "99788618",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eec46d27"
      },
      "source": [
        "model_filename_no_pooling = os.path.join(models_dir, 'model_no_pooling.keras')\n",
        "\n",
        "# Load the 'no_pooling' model\n",
        "model_no_pooling = keras.models.load_model(model_filename_no_pooling)\n",
        "\n",
        "# Evaluate the loaded 'no_pooling' model on the test_generator\n",
        "test_loss_no_pooling, test_acc_no_pooling = model_no_pooling.evaluate(test_generator, verbose=0)\n",
        "\n",
        "# Store the obtained test accuracy and test loss in the test_results dictionary\n",
        "test_results['no_pooling'] = {'test_acc': test_acc_no_pooling, 'test_loss': test_loss_no_pooling}\n",
        "\n",
        "print(f\"No-Pooling Model - Test Accuracy: {test_acc_no_pooling:.4f}\")\n",
        "print(f\"No-Pooling Model - Test Loss: {test_loss_no_pooling:.4f}\")\n",
        "print(\"Test results for 'no_pooling' model stored.\")"
      ],
      "id": "eec46d27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60235235"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow import keras\n",
        "\n",
        "# Make sure models_dir and test_generator are defined from previous cells\n",
        "# models_dir = '/content/drive/MyDrive/models'\n",
        "# test_generator (already available)\n",
        "# results_pooling_comparison (already available from previous cell)\n",
        "\n",
        "# test_results dictionary already populated from previous steps\n",
        "\n",
        "for model_name_key in results_pooling_comparison.keys():\n",
        "    print(f\"\\n{'='*60}\\nEvaluating and Plotting {model_name_key} model\\n{'='*60}\")\n",
        "\n",
        "    model_filename = os.path.join(models_dir, f'model_{model_name_key}.keras')\n",
        "    model = keras.models.load_model(model_filename)\n",
        "\n",
        "    # 1. Evaluate on Test Data (Re-evaluating to get the values in this block for consistency, though already done)\n",
        "    print(f\"Evaluating {model_name_key} model on test data...\")\n",
        "    test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    test_results[model_name_key] = {'test_acc': test_acc, 'test_loss': test_loss}\n",
        "\n",
        "    # Retrieve history for plotting\n",
        "    history = results_pooling_comparison[model_name_key]\n",
        "\n",
        "    # 2. Plotting (Accuracy, Loss over Epochs, Val vs Test Comparison)\n",
        "    acc = history['accuracy']\n",
        "    val_acc = history['val_accuracy']\n",
        "    loss = history['loss']\n",
        "    val_loss = history['val_loss']\n",
        "    epochs_range = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    # -- Graph 1: Accuracy over Epochs --\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs_range, acc, 'bo-', label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, 'r^-', label='Validation Accuracy')\n",
        "    plt.title(f'{model_name_key.replace(\"_\", \" \").capitalize()} Model - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, 1) # Ensure y-axis range from 0 to 1 for accuracy\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # -- Graph 2: Loss over Epochs --\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs_range, loss, 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, 'r^-', label='Validation Loss')\n",
        "    plt.title(f'{model_name_key.replace(\"_\", \" \").capitalize()} Model - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # -- Graph 3: Validation vs Test Comparison --\n",
        "    plt.subplot(1, 3, 3)\n",
        "    metrics_names = ['Accuracy', 'Loss']\n",
        "    val_values = [val_acc[-1], val_loss[-1]]\n",
        "    current_test_values = [test_acc, test_loss]\n",
        "\n",
        "    x_pos = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "\n",
        "    rects1 = plt.bar(x_pos - width/2, val_values, width, label='Validation')\n",
        "    rects2 = plt.bar(x_pos + width/2, current_test_values, width, label='Test')\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(f'{model_name_key.replace(\"_\", \" \").capitalize()} - Val vs Test')\n",
        "    plt.xticks(x_pos, metrics_names)\n",
        "    plt.legend()\n",
        "\n",
        "    # Add labels\n",
        "    plt.bar_label(rects1, padding=3, fmt='%.4f')\n",
        "    plt.bar_label(rects2, padding=3, fmt='%.4f')\n",
        "    plt.grid(axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nComparison of test metrics for both models:\")\n",
        "for model_name, metrics in test_results.items():\n",
        "    print(f\"{model_name.capitalize()} Model: Test Accuracy = {metrics['test_acc']:.4f}, Test Loss = {metrics['test_loss']:.4f}\")\n",
        "\n",
        "# Summarize Comparison\n",
        "print(\"\\n--- Summary of Pooling vs. No-Pooling Models ---\")\n",
        "pooling_acc = test_results['pooling']['test_acc']\n",
        "pooling_loss = test_results['pooling']['test_loss']\n",
        "no_pooling_acc = test_results['no_pooling']['test_acc']\n",
        "no_pooling_loss = test_results['no_pooling']['test_loss']\n",
        "\n",
        "print(f\"Pooling Model: Test Accuracy = {pooling_acc:.4f}, Test Loss = {pooling_loss:.4f}\")\n",
        "print(f\"No-Pooling Model: Test Accuracy = {no_pooling_acc:.4f}, Test Loss = {no_pooling_loss:.4f}\")\n",
        "\n",
        "if pooling_acc > no_pooling_acc:\n",
        "    print(f\"\\nThe Pooling Model achieved higher test accuracy ({pooling_acc:.4f}) compared to the No-Pooling Model ({no_pooling_acc:.4f}).\")\n",
        "    print(\"This suggests that pooling layers were beneficial in extracting robust features and reducing overfitting.\")\n",
        "elif no_pooling_acc > pooling_acc:\n",
        "    print(f\"\\nThe No-Pooling Model achieved higher test accuracy ({no_pooling_acc:.4f}) compared to the Pooling Model ({pooling_acc:.4f}).\")\n",
        "    print(\"However, looking at the training and validation curves, the no-pooling model shows significant overfitting (high training accuracy, low validation accuracy), which is a common issue when pooling is absent, as it retains too much spatial information.\")\n",
        "else:\n",
        "    print(\"\\nBoth models achieved similar test accuracy.\")\n",
        "\n",
        "print(\"\\nKey observations from plots:\")\n",
        "print(\"- **Pooling Model**: The training and validation accuracy/loss curves show a relatively stable learning process, with validation metrics generally following training metrics, indicating less overfitting.\")\n",
        "print(\"- **No-Pooling Model**: The training accuracy quickly reaches very high levels (near 1.0) while validation accuracy plateaus or even decreases, and validation loss increases significantly. This is a clear indication of severe overfitting, where the model learns the training data too well but fails to generalize to unseen data.\")\n",
        "print(\"\\nConclusion: Pooling layers (MaxPooling2D in this case) are crucial for image classification tasks. They help in down-sampling feature maps, reducing computational cost, and providing translation invariance, which significantly improves the model's generalization capabilities and prevents overfitting, as evidenced by the much better performance and convergence behavior of the pooling model.\")\n"
      ],
      "id": "60235235",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that was insightful. Here I can clearly see that removing pooling leads to high overfitting to training data, despite theoretically preserving all data throughout the training."
      ],
      "metadata": {
        "id": "CSiPOgI3k0cA"
      },
      "id": "CSiPOgI3k0cA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Googling: what is skip connection in CNN?\n",
        "Result: Skip connections, or residual connections, in CNNs are shortcuts that bypass one or more layers to feed the input of an earlier layer directly to the output of a later layer. This helps train deeper networks by addressing the vanishing gradient problem, allowing gradients to flow more easily through the network during backpropagation.\n",
        "Ahh, I think I remember now. Do I need a big number of layers to get pronounced vanishing gradients? I'm not using very deep nets up until now."
      ],
      "metadata": {
        "id": "aSJR4pm251P3"
      },
      "id": "aSJR4pm251P3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimenting with skipping connections"
      ],
      "metadata": {
        "id": "1YCU-PqTjD_W"
      },
      "id": "1YCU-PqTjD_W"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53d40bab"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "results_skip_comparison = {}\n",
        "\n",
        "# Define the 'sequential' CNN model\n",
        "model_sequential = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "model_sequential.compile(optimizer='adam',\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "# Define the 'skip_connection' CNN model using Keras Functional API\n",
        "input_tensor = keras.Input(shape=(64, 64, 3))\n",
        "\n",
        "# First Conv block\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu')(input_tensor)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "skip_connection_branch = x  # Output: (31, 31, 32) (assuming 'valid' padding for Conv2D)\n",
        "\n",
        "# Second Conv block with skip connection\n",
        "# Main path\n",
        "y = layers.Conv2D(64, (3, 3), activation='relu')(x)  # Output: (29, 29, 64)\n",
        "\n",
        "# Skip path: needs to match shape of y (29, 29, 64)\n",
        "# Apply a Conv2D with kernel_size=(3,3) and 'valid' padding to reduce spatial dimensions\n",
        "# and increase channels to match 'y'. Activation can be linear or relu for the skip path.\n",
        "skip_connection_branch_processed = layers.Conv2D(64, (3, 3), activation='relu')(skip_connection_branch)\n",
        "\n",
        "# Add the main path and skip path\n",
        "z = layers.add([y, skip_connection_branch_processed])\n",
        "z = layers.MaxPooling2D((2, 2))(z) # Output: (14, 14, 64)\n",
        "\n",
        "# Third Conv block\n",
        "z = layers.Conv2D(64, (3, 3), activation='relu')(z)\n",
        "z = layers.MaxPooling2D((2, 2))(z) # Output: (6, 6, 64)\n",
        "\n",
        "# Classifier head\n",
        "z = layers.Flatten()(z)\n",
        "z = layers.Dense(64, activation='relu')(z)\n",
        "output_tensor = layers.Dense(8, activation='softmax')(z)\n",
        "\n",
        "model_skip_connection = keras.Model(inputs=input_tensor, outputs=output_tensor, name='skip_connection_model')\n",
        "\n",
        "model_skip_connection.compile(optimizer='adam',\n",
        "                              loss='sparse_categorical_crossentropy',\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "models_to_train = {\n",
        "    'sequential': model_sequential,\n",
        "    'skip_connection': model_skip_connection\n",
        "}\n",
        "\n",
        "for model_name_key, model_obj in models_to_train.items():\n",
        "    print(f\"\\n{'='*60}\\nProcessing {model_name_key} model\\n{'='*60}\")\n",
        "\n",
        "    model_filename = os.path.join(models_dir, f'model_{model_name_key}.keras')\n",
        "    history_filename = os.path.join(models_dir, f'history_{model_name_key}.pkl')\n",
        "\n",
        "    if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "        print(f\"Loading existing model and history for {model_name_key}...\")\n",
        "        # Ensure to load into model_obj if it was defined as a placeholder, or assign to a new variable\n",
        "        # For this setup, we'll reload and use the loaded model for evaluation/plotting later\n",
        "        loaded_model = keras.models.load_model(model_filename) # Load, but the model_obj itself was already compiled\n",
        "        with open(history_filename, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "    else:\n",
        "        print(f\"Training {model_name_key} model...\")\n",
        "        history_obj = model_obj.fit(\n",
        "            train_generator,\n",
        "            epochs=10,\n",
        "            validation_data=validation_generator,\n",
        "            verbose=1\n",
        "        )\n",
        "        history = history_obj.history\n",
        "\n",
        "        # Save model and history\n",
        "        model_obj.save(model_filename)\n",
        "        with open(history_filename, 'wb') as f:\n",
        "            pickle.dump(history, f)\n",
        "        print(f\"Saved model and history for {model_name_key}.\")\n",
        "\n",
        "    # Store history for later plotting\n",
        "    results_skip_comparison[model_name_key] = history\n",
        "\n",
        "print(\"Models defined, trained (or loaded), and histories saved/stored in results_skip_comparison.\")"
      ],
      "id": "53d40bab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c78bdaac"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow import keras\n",
        "\n",
        "# Ensure models_dir and test_generator are defined from previous cells\n",
        "# models_dir = '/content/drive/MyDrive/models'\n",
        "# test_generator (already available)\n",
        "# results_skip_comparison (already available from previous cell)\n",
        "\n",
        "# Initialize test_results for skip connection comparison if not already done\n",
        "# (It might have been populated by the previous 'pooling' comparison, but it's safer to re-initialize or ensure proper keys)\n",
        "comparison_test_results = {}\n",
        "\n",
        "for model_name_key in results_skip_comparison.keys():\n",
        "    print(f\"\\n{'='*60}\\nEvaluating and Plotting {model_name_key} model\\n{'='*60}\")\n",
        "\n",
        "    model_filename = os.path.join(models_dir, f'model_{model_name_key}.keras')\n",
        "    model = keras.models.load_model(model_filename)\n",
        "\n",
        "    # 1. Evaluate on Test Data\n",
        "    print(f\"Evaluating {model_name_key} model on test data...\")\n",
        "    test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    comparison_test_results[model_name_key] = {'test_acc': test_acc, 'test_loss': test_loss}\n",
        "\n",
        "    # Retrieve history for plotting\n",
        "    history = results_skip_comparison[model_name_key]\n",
        "\n",
        "    # 2. Plotting (Accuracy, Loss over Epochs, Val vs Test Comparison)\n",
        "    acc = history['accuracy']\n",
        "    val_acc = history['val_accuracy']\n",
        "    loss = history['loss']\n",
        "    val_loss = history['val_loss']\n",
        "    epochs_range = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    # -- Graph 1: Accuracy over Epochs --\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs_range, acc, 'bo-', label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, 'r^-', label='Validation Accuracy')\n",
        "    plt.title(f'{model_name_key.replace(\"_\", \" \").capitalize()} Model - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, 1) # Ensure y-axis range from 0 to 1 for accuracy\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # -- Graph 2: Loss over Epochs --\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs_range, loss, 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, 'r^-', label='Validation Loss')\n",
        "    plt.title(f'{model_name_key.replace(\"_\", \" \").capitalize()} Model - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # -- Graph 3: Validation vs Test Comparison --\n",
        "    plt.subplot(1, 3, 3)\n",
        "    metrics_names = ['Accuracy', 'Loss']\n",
        "    val_values = [val_acc[-1], val_loss[-1]]\n",
        "    current_test_values = [test_acc, test_loss]\n",
        "\n",
        "    x_pos = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "\n",
        "    rects1 = plt.bar(x_pos - width/2, val_values, width, label='Validation')\n",
        "    rects2 = plt.bar(x_pos + width/2, current_test_values, width, label='Test')\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(f'{model_name_key.replace(\"_\", \" \").capitalize()} - Val vs Test')\n",
        "    plt.xticks(x_pos, metrics_names)\n",
        "    plt.legend()\n",
        "\n",
        "    # Add labels\n",
        "    plt.bar_label(rects1, padding=3, fmt='%.4f')\n",
        "    plt.bar_label(rects2, padding=3, fmt='%.4f')\n",
        "    plt.grid(axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nComparison of test metrics for both models:\")\n",
        "for model_name, metrics in comparison_test_results.items():\n",
        "    print(f\"{model_name.capitalize()} Model: Test Accuracy = {metrics['test_acc']:.4f}, Test Loss = {metrics['test_loss']:.4f}\")\n",
        "\n",
        "# Summarize Comparison\n",
        "print(\"\\n--- Summary of Sequential vs. Skip Connection Models ---\")\n",
        "sequential_acc = comparison_test_results['sequential']['test_acc']\n",
        "sequential_loss = comparison_test_results['sequential']['test_loss']\n",
        "skip_acc = comparison_test_results['skip_connection']['test_acc']\n",
        "skip_loss = comparison_test_results['skip_connection']['test_loss']\n",
        "\n",
        "print(f\"Sequential Model: Test Accuracy = {sequential_acc:.4f}, Test Loss = {sequential_loss:.4f}\")\n",
        "print(f\"Skip_connection Model: Test Accuracy = {skip_acc:.4f}, Test Loss = {skip_loss:.4f}\")\n",
        "\n",
        "if skip_acc > sequential_acc:\n",
        "    print(f\"\\nThe Skip Connection Model achieved higher test accuracy ({skip_acc:.4f}) compared to the Sequential Model ({sequential_acc:.4f}).\")\n",
        "    print(\"This suggests that incorporating skip connections can be beneficial for improving model performance and potentially alleviating vanishing gradient problems in deeper networks.\")\n",
        "elif sequential_acc > skip_acc:\n",
        "    print(f\"\\nThe Sequential Model achieved higher test accuracy ({sequential_acc:.4f}) compared to the Skip Connection Model ({skip_acc:.4f}).\")\n",
        "    print(\"This result is unexpected and might indicate issues with the skip connection implementation, hyperparameter tuning, or dataset characteristics.\")\n",
        "else:\n",
        "    print(\"\\nBoth models achieved similar test accuracy.\")\n",
        "\n",
        "print(\"\\nKey observations from plots (based on typical behavior, may vary with specific training runs):\")\n",
        "print(\"- **Sequential Model**: Observe the trends in training and validation accuracy/loss to identify signs of overfitting or underfitting.\")\n",
        "print(\"- **Skip Connection Model**: Observe the trends, typically expecting smoother learning curves and better generalization due to skip connections facilitating gradient flow.\")\n",
        "print(\"\\nFurther analysis might involve:\")\n",
        "print(\"- Examining the complexity of the skip connection branch to ensure it's not adding unnecessary parameters or complexity.\")\n",
        "print(\"- Experimenting with different skip connection architectures (e.g., identity mapping, convolutional layers with 1x1 kernels).\")\n",
        "print(\"- Adjusting learning rates, optimizers, or regularization techniques to optimize both models.\")\n"
      ],
      "id": "c78bdaac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skip connections doesn't seem so much different from sequential in this case. Probably need way more layers to see the difference?"
      ],
      "metadata": {
        "id": "aeIwXCkj5qet"
      },
      "id": "aeIwXCkj5qet"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with different number of hidden layers"
      ],
      "metadata": {
        "id": "JTEHGdHAjQWU"
      },
      "id": "JTEHGdHAjQWU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3b0450d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Define the specific layer counts requested\n",
        "dense_layer_counts = [1, 5, 9]\n",
        "results_dense_layers = {}\n",
        "\n",
        "for count in dense_layer_counts:\n",
        "    print(f\"\\n{'='*60}\\nProcessing model with {count} Hidden Dense Layer(s)\\n{'='*60}\")\n",
        "\n",
        "    # 1. Define Model\n",
        "    model = models.Sequential()\n",
        "    # Fixed Convolutional Base\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Variable Number of Fully Connected (Dense) Layers\n",
        "    for _ in range(count):\n",
        "        model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(layers.Dense(8, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 2. Load or Train\n",
        "    model_name = f'dense_layers_{count}'\n",
        "    model_filename = os.path.join(models_dir, f'model_{model_name}.keras')\n",
        "    history_filename = os.path.join(models_dir, f'history_{model_name}.pkl')\n",
        "\n",
        "    if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "        print(f\"Loading existing model and history for {model_name}...\")\n",
        "        model = keras.models.load_model(model_filename)\n",
        "        with open(history_filename, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "    else:\n",
        "        print(f\"Training {model_name}...\")\n",
        "        history_obj = model.fit(\n",
        "            train_generator,\n",
        "            epochs=10,\n",
        "            validation_data=validation_generator,\n",
        "            verbose=1\n",
        "        )\n",
        "        history = history_obj.history\n",
        "\n",
        "        # Save model and history\n",
        "        model.save(model_filename)\n",
        "        with open(history_filename, 'wb') as f:\n",
        "            pickle.dump(history, f)\n",
        "        print(f\"Saved model and history for {model_name}.\")\n",
        "\n",
        "    # 3. Evaluate on Test Data\n",
        "    print(f\"Evaluating {model_name} on test data...\")\n",
        "    test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    results_dense_layers[count] = {'test_acc': test_acc, 'test_loss': test_loss, 'history': history}\n",
        "\n",
        "    # 4. Plotting Individual Model Performance\n",
        "    acc = history['accuracy']\n",
        "    val_acc = history['val_accuracy']\n",
        "    loss = history['loss']\n",
        "    val_loss = history['val_loss']\n",
        "    epochs_range = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, 'bo-', label='Training acc')\n",
        "    plt.plot(epochs_range, val_acc, 'r^-', label='Validation acc')\n",
        "    plt.title(f'{count} Hidden Dense Layer(s) - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, 'bo-', label='Training loss')\n",
        "    plt.plot(epochs_range, val_loss, 'r^-', label='Validation loss')\n",
        "    plt.title(f'{count} Hidden Dense Layer(s) - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 5. Compare All Models\n",
        "print(f\"\\n{'='*60}\\nComparison of Dense Layer Configurations (1, 5, 9)\\n{'='*60}\")\n",
        "\n",
        "metrics_names = ['Accuracy', 'Loss']\n",
        "labels = [f'{c} Layers' for c in dense_layer_counts]\n",
        "accuracies = [results_dense_layers[c]['test_acc'] for c in dense_layer_counts]\n",
        "losses = [results_dense_layers[c]['test_loss'] for c in dense_layer_counts]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "rects1 = plt.bar(x - width/2, accuracies, width, label='Test Accuracy')\n",
        "rects2 = plt.bar(x + width/2, losses, width, label='Test Loss')\n",
        "\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Test Metrics by Number of Hidden Dense Layers')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "\n",
        "plt.bar_label(rects1, padding=3, fmt='%.4f')\n",
        "plt.bar_label(rects2, padding=3, fmt='%.4f')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print Summary Text\n",
        "for count in dense_layer_counts:\n",
        "    print(f\"{count} Hidden Dense Layer(s): Test Accuracy = {results_dense_layers[count]['test_acc']:.4f}, Test Loss = {results_dense_layers[count]['test_loss']:.4f}\")"
      ],
      "id": "a3b0450d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can see some better results in minimising loss after a bigger number of layers.\n",
        "If I'll have some time left I want to return to this excercise to see how deep can I go >.>"
      ],
      "metadata": {
        "id": "z3fH2517_jPN"
      },
      "id": "z3fH2517_jPN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset variations"
      ],
      "metadata": {
        "id": "NXYcpZdA_6Nf"
      },
      "id": "NXYcpZdA_6Nf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 channel vs 3 channels"
      ],
      "metadata": {
        "id": "PuNPSR9Y_9iD"
      },
      "id": "PuNPSR9Y_9iD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google: what is a channel in a dataset?\n",
        "Result: In a dataset, a channel represents a layer of information for a single data point, such as a single pixel in an image. A 1-channel dataset, like a grayscale image, has a single value per pixel to represent its intensity, while a 3-channel dataset, like a standard RGB color image, has three values per pixel for the red, green, and blue components.\n"
      ],
      "metadata": {
        "id": "vK-kmeHdALSN"
      },
      "id": "vK-kmeHdALSN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh I see, I was a bit confused because I didn't work with images before. I get it now."
      ],
      "metadata": {
        "id": "B7JWSMT8ANqb"
      },
      "id": "B7JWSMT8ANqb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1910a644"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# 1. Define Generators\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# RGB Generators\n",
        "print(\"Creating RGB Generators...\")\n",
        "train_gen_rgb = datagen.flow_from_dataframe(\n",
        "    train, x_col='file', y_col='age_bins', target_size=(64, 64),\n",
        "    batch_size=32, class_mode='sparse', color_mode='rgb'\n",
        ")\n",
        "val_gen_rgb = datagen.flow_from_dataframe(\n",
        "    validate, x_col='file', y_col='age_bins', target_size=(64, 64),\n",
        "    batch_size=32, class_mode='sparse', color_mode='rgb'\n",
        ")\n",
        "test_gen_rgb = datagen.flow_from_dataframe(\n",
        "    test, x_col='file', y_col='age_bins', target_size=(64, 64),\n",
        "    batch_size=32, class_mode='sparse', color_mode='rgb'\n",
        ")\n",
        "\n",
        "# Grayscale Generators\n",
        "print(\"Creating Grayscale Generators...\")\n",
        "train_gen_gray = datagen.flow_from_dataframe(\n",
        "    train, x_col='file', y_col='age_bins', target_size=(64, 64),\n",
        "    batch_size=32, class_mode='sparse', color_mode='grayscale'\n",
        ")\n",
        "val_gen_gray = datagen.flow_from_dataframe(\n",
        "    validate, x_col='file', y_col='age_bins', target_size=(64, 64),\n",
        "    batch_size=32, class_mode='sparse', color_mode='grayscale'\n",
        ")\n",
        "test_gen_gray = datagen.flow_from_dataframe(\n",
        "    test, x_col='file', y_col='age_bins', target_size=(64, 64),\n",
        "    batch_size=32, class_mode='sparse', color_mode='grayscale'\n",
        ")\n",
        "\n",
        "# 2. Define Model Creation Function\n",
        "def create_model(input_shape):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(8, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 3. Experiment Loop\n",
        "configs = [\n",
        "    {'name': 'rgb', 'shape': (64, 64, 3), 'train': train_gen_rgb, 'val': val_gen_rgb, 'test': test_gen_rgb},\n",
        "    {'name': 'grayscale', 'shape': (64, 64, 1), 'train': train_gen_gray, 'val': val_gen_gray, 'test': test_gen_gray}\n",
        "]\n",
        "\n",
        "results_channels = {}\n",
        "\n",
        "for config in configs:\n",
        "    name = config['name']\n",
        "    print(f\"\\n{'='*60}\\nProcessing {name} model\\n{'='*60}\")\n",
        "\n",
        "    model_filename = os.path.join(models_dir, f'model_channel_{name}.keras')\n",
        "    history_filename = os.path.join(models_dir, f'history_channel_{name}.pkl')\n",
        "\n",
        "    if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "        print(f\"Loading existing model and history for {name}...\")\n",
        "        model = models.load_model(model_filename)\n",
        "        with open(history_filename, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "    else:\n",
        "        print(f\"Training {name} model...\")\n",
        "        model = create_model(config['shape'])\n",
        "        history_obj = model.fit(\n",
        "            config['train'],\n",
        "            epochs=10,\n",
        "            validation_data=config['val'],\n",
        "            verbose=1\n",
        "        )\n",
        "        history = history_obj.history\n",
        "\n",
        "        # Save\n",
        "        model.save(model_filename)\n",
        "        with open(history_filename, 'wb') as f:\n",
        "            pickle.dump(history, f)\n",
        "        print(f\"Saved model and history for {name}.\")\n",
        "\n",
        "    # Evaluate\n",
        "    print(f\"Evaluating {name} model...\")\n",
        "    loss, acc = model.evaluate(config['test'], verbose=0)\n",
        "    print(f\"Test Accuracy: {acc:.4f}, Test Loss: {loss:.4f}\")\n",
        "    results_channels[name] = {'acc': acc, 'loss': loss, 'history': history}\n",
        "\n",
        "# 4. Plotting Results\n",
        "# Bar Chart Comparison\n",
        "names = list(results_channels.keys())\n",
        "accs = [results_channels[n]['acc'] for n in names]\n",
        "losses = [results_channels[n]['loss'] for n in names]\n",
        "\n",
        "x = np.arange(len(names))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "rects1 = plt.bar(x - width/2, accs, width, label='Accuracy')\n",
        "rects2 = plt.bar(x + width/2, losses, width, label='Loss')\n",
        "plt.xlabel('Channel Configuration')\n",
        "plt.ylabel('Score')\n",
        "plt.title('RGB vs Grayscale Test Metrics')\n",
        "plt.xticks(x, names)\n",
        "plt.legend()\n",
        "plt.bar_label(rects1, padding=3, fmt='%.4f')\n",
        "plt.bar_label(rects2, padding=3, fmt='%.4f')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Line Plots Comparison\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "for name in names:\n",
        "    hist = results_channels[name]['history']\n",
        "    plt.plot(hist['accuracy'], label=f'{name} Train')\n",
        "    plt.plot(hist['val_accuracy'], linestyle='--', label=f'{name} Val')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Loss Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "for name in names:\n",
        "    hist = results_channels[name]['history']\n",
        "    plt.plot(hist['loss'], label=f'{name} Train')\n",
        "    plt.plot(hist['val_loss'], linestyle='--', label=f'{name} Val')\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "1910a644",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm not sure why it needed to reinitialize RGB dataset. Anyway, I'd probably spent more time arguing with it, so I'll just roll with it for now."
      ],
      "metadata": {
        "id": "_ztD9Eg9EjdG"
      },
      "id": "_ztD9Eg9EjdG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "RGB accuracy and validation looks a lit bit better. I'm not surprized, to be fair, given that three channels convey more information.\n",
        "Anyway, it was a good excersize just to try it out myself."
      ],
      "metadata": {
        "id": "b0pEkU2UFoxC"
      },
      "id": "b0pEkU2UFoxC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt: what are the ways to augment image data?"
      ],
      "metadata": {
        "id": "kjuklRMejdR6"
      },
      "id": "kjuklRMejdR6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f2e65c8"
      },
      "source": [
        "Responce:\n",
        "## Data Augmentation\n",
        "\n",
        "Data augmentation helps prevent overfitting by exposing the model to more variations of the training data. We can use `ImageDataGenerator` to apply transformations on the fly during training.\n",
        "\n",
        "Below, we define a generator with:\n",
        "*   Rotation\n",
        "*   Width/Height Shifts\n",
        "*   Shear\n",
        "*   Zoom\n",
        "*   Horizontal Flip"
      ],
      "id": "0f2e65c8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e21f6b6"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# 1. Select a sample image\n",
        "sample_file = train['file'].iloc[0]\n",
        "print(f\"Visualizing augmentations for: {sample_file}\")\n",
        "\n",
        "# Load and convert to array\n",
        "img = load_img(sample_file, target_size=(64, 64))\n",
        "x = img_to_array(img)\n",
        "x = x.reshape((1,) + x.shape)  # Reshape to (1, 64, 64, 3)\n",
        "\n",
        "# 2. Define Augmentation Generator\n",
        "aug_datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# 3. Generate and Plot Augmented Images\n",
        "i = 0\n",
        "plt.figure(figsize=(12, 6))\n",
        "for batch in aug_datagen.flow(x, batch_size=1):\n",
        "    plt.subplot(2, 4, i + 1)\n",
        "    imgplot = plt.imshow(batch[0].astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    i += 1\n",
        "    if i % 8 == 0:\n",
        "        break\n",
        "\n",
        "plt.suptitle(\"Example of Data Augmentations\")\n",
        "plt.show()"
      ],
      "id": "3e21f6b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26c14ec2"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 1. Define Augmented Training Generator\n",
        "# Note: rescale=1./255 is applied to both augmented and validation/test generators\n",
        "train_datagen_aug = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# 2. Define Validation/Test Generator (No Augmentation, only rescale)\n",
        "valid_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"Setting up Augmented Training Generator...\")\n",
        "train_generator_aug = train_datagen_aug.flow_from_dataframe(\n",
        "    dataframe=train,\n",
        "    x_col='file',\n",
        "    y_col='age_bins',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb'\n",
        ")\n",
        "\n",
        "print(\"Setting up Validation and Test Generators...\")\n",
        "validation_generator_aug = valid_test_datagen.flow_from_dataframe(\n",
        "    dataframe=validate,\n",
        "    x_col='file',\n",
        "    y_col='age_bins',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb'\n",
        ")\n",
        "\n",
        "test_generator_aug = valid_test_datagen.flow_from_dataframe(\n",
        "    dataframe=test,\n",
        "    x_col='file',\n",
        "    y_col='age_bins',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb'\n",
        ")"
      ],
      "id": "26c14ec2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a model on augmented data"
      ],
      "metadata": {
        "id": "vwn4QtkpjoK7"
      },
      "id": "vwn4QtkpjoK7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba6c7f94"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# 1. Define Model Architecture (Same as RGB baseline)\n",
        "model_aug = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "model_aug.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# 2. Load or Train Check\n",
        "model_filename = os.path.join(models_dir, 'model_aug.keras')\n",
        "history_filename = os.path.join(models_dir, 'history_aug.pkl')\n",
        "\n",
        "if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "    print(\"Loading existing augmented model and history...\")\n",
        "    model_aug = keras.models.load_model(model_filename)\n",
        "    with open(history_filename, 'rb') as f:\n",
        "        history_aug = pickle.load(f)\n",
        "else:\n",
        "    print(\"Training augmented model...\")\n",
        "    history_obj = model_aug.fit(\n",
        "        train_generator_aug,\n",
        "        epochs=10,\n",
        "        validation_data=validation_generator_aug,\n",
        "        verbose=1\n",
        "    )\n",
        "    history_aug = history_obj.history\n",
        "\n",
        "    # Save\n",
        "    model_aug.save(model_filename)\n",
        "    with open(history_filename, 'wb') as f:\n",
        "        pickle.dump(history_aug, f)\n",
        "    print(\"Saved augmented model and history.\")\n",
        "\n",
        "# 3. Evaluate on Test Data\n",
        "print(\"Evaluating augmented model on test data...\")\n",
        "test_loss_aug, test_acc_aug = model_aug.evaluate(test_generator_aug, verbose=0)\n",
        "\n",
        "# 4. Comparison Visualization\n",
        "print(f\"\\n{'='*60}\\nComparison: Augmented vs Baseline RGB\\n{'='*60}\")\n",
        "\n",
        "# Retrieve Baseline Results\n",
        "baseline_acc = results_channels['rgb']['acc']\n",
        "baseline_loss = results_channels['rgb']['loss']\n",
        "\n",
        "labels = ['Baseline RGB', 'Augmented']\n",
        "accuracies = [baseline_acc, test_acc_aug]\n",
        "losses = [baseline_loss, test_loss_aug]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "rects1 = plt.bar(x - width/2, accuracies, width, label='Test Accuracy')\n",
        "rects2 = plt.bar(x + width/2, losses, width, label='Test Loss')\n",
        "\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Test Metrics: Baseline RGB vs Augmented')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.bar_label(rects1, padding=3, fmt='%.4f')\n",
        "plt.bar_label(rects2, padding=3, fmt='%.4f')\n",
        "plt.show()\n",
        "\n",
        "# 5. Learning Curves for Augmented Model\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_aug['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_aug['val_accuracy'], linestyle='--', label='Val Accuracy')\n",
        "plt.title('Augmented Model - Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_aug['loss'], label='Train Loss')\n",
        "plt.plot(history_aug['val_loss'], linestyle='--', label='Val Loss')\n",
        "plt.title('Augmented Model - Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Print Summary\n",
        "print(f\"Baseline RGB Model: Test Accuracy = {baseline_acc:.4f}, Test Loss = {baseline_loss:.4f}\")\n",
        "print(f\"Augmented Model:    Test Accuracy = {test_acc_aug:.4f}, Test Loss = {test_loss_aug:.4f}\")"
      ],
      "id": "ba6c7f94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model based on augmented data show less accuracy and bigger loss after the training. That is interesting. Guess augmentation can be usefull still for standartized datasets that would be used for inference from non-standartized data."
      ],
      "metadata": {
        "id": "rKLa9JbxJVhS"
      },
      "id": "rKLa9JbxJVhS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying out regression inference of age (as opposed to categorical)"
      ],
      "metadata": {
        "id": "mNYSoD3vjyAo"
      },
      "id": "mNYSoD3vjyAo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9b0ad04"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 1. Setup Regression Generators (Target: 'age')\n",
        "# We use the 'raw' class_mode for regression targets\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"Setting up Regression Generators...\")\n",
        "# Using 'age' column which contains integer values\n",
        "train_gen_reg = datagen.flow_from_dataframe(\n",
        "    dataframe=train, x_col='file', y_col='age',\n",
        "    target_size=(64, 64), batch_size=32, class_mode='raw', color_mode='rgb'\n",
        ")\n",
        "\n",
        "val_gen_reg = datagen.flow_from_dataframe(\n",
        "    dataframe=validate, x_col='file', y_col='age',\n",
        "    target_size=(64, 64), batch_size=32, class_mode='raw', color_mode='rgb'\n",
        ")\n",
        "\n",
        "test_gen_reg = datagen.flow_from_dataframe(\n",
        "    dataframe=test, x_col='file', y_col='age',\n",
        "    target_size=(64, 64), batch_size=32, class_mode='raw', color_mode='rgb'\n",
        ")\n",
        "\n",
        "# 2. Define Regression Model\n",
        "def create_regression_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        # Output layer: 1 unit with linear activation for regression\n",
        "        layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='mse',  # Mean Squared Error for regression\n",
        "                  metrics=['mae']) # Mean Absolute Error\n",
        "    return model\n",
        "\n",
        "# 3. Train or Load\n",
        "model_name = 'regression'\n",
        "model_filename = os.path.join(models_dir, f'model_{model_name}.keras')\n",
        "history_filename = os.path.join(models_dir, f'history_{model_name}.pkl')\n",
        "\n",
        "if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "    print(f\"Loading existing model and history for {model_name}...\")\n",
        "    model_reg = keras.models.load_model(model_filename)\n",
        "    with open(history_filename, 'rb') as f:\n",
        "        history_reg = pickle.load(f)\n",
        "else:\n",
        "    print(f\"Training {model_name} model...\")\n",
        "    model_reg = create_regression_model()\n",
        "    history_obj = model_reg.fit(\n",
        "        train_gen_reg,\n",
        "        epochs=10,\n",
        "        validation_data=val_gen_reg,\n",
        "        verbose=1\n",
        "    )\n",
        "    history_reg = history_obj.history\n",
        "\n",
        "    # Save\n",
        "    model_reg.save(model_filename)\n",
        "    with open(history_filename, 'wb') as f:\n",
        "        pickle.dump(history_reg, f)\n",
        "    print(f\"Saved model and history for {model_name}.\")\n",
        "\n",
        "# 4. Evaluate\n",
        "print(\"Evaluating regression model on test data...\")\n",
        "test_mse, test_mae = model_reg.evaluate(test_gen_reg, verbose=0)\n",
        "print(f\"Test MSE: {test_mse:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "# 5. Visualization\n",
        "# History\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_reg['mae'], label='Train MAE')\n",
        "plt.plot(history_reg['val_mae'], label='Val MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE (Years)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_reg['loss'], label='Train MSE')\n",
        "plt.plot(history_reg['val_loss'], label='Val MSE')\n",
        "plt.title('Model Loss (MSE)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Predictions vs Actual\n",
        "# Create a separate generator without shuffling to match predictions with true values\n",
        "test_gen_reg_no_shuffle = datagen.flow_from_dataframe(\n",
        "    dataframe=test, x_col='file', y_col='age',\n",
        "    target_size=(64, 64), batch_size=32, class_mode='raw', color_mode='rgb', shuffle=False\n",
        ")\n",
        "\n",
        "predictions = model_reg.predict(test_gen_reg_no_shuffle)\n",
        "true_values = test['age'].values\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(true_values, predictions, alpha=0.5)\n",
        "# Plot ideal prediction line\n",
        "max_age = max(max(true_values), max(predictions.flatten()))\n",
        "plt.plot([0, max_age], [0, max_age], 'r--', label='Ideal Fit')\n",
        "plt.xlabel('True Age')\n",
        "plt.ylabel('Predicted Age')\n",
        "plt.title('True vs Predicted Age (Regression)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "c9b0ad04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying out different final layer activation functions"
      ],
      "metadata": {
        "id": "Gb1J21b5j81C"
      },
      "id": "Gb1J21b5j81C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebf948a9"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# 1. Initialize Generators\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"Setting up Generators...\")\n",
        "train_gen = datagen.flow_from_dataframe(\n",
        "    dataframe=train, x_col='file', y_col='age_bins',\n",
        "    target_size=(64, 64), batch_size=32, class_mode='sparse', color_mode='rgb'\n",
        ")\n",
        "\n",
        "val_gen = datagen.flow_from_dataframe(\n",
        "    dataframe=validate, x_col='file', y_col='age_bins',\n",
        "    target_size=(64, 64), batch_size=32, class_mode='sparse', color_mode='rgb'\n",
        ")\n",
        "\n",
        "test_gen = datagen.flow_from_dataframe(\n",
        "    dataframe=test, x_col='file', y_col='age_bins',\n",
        "    target_size=(64, 64), batch_size=32, class_mode='sparse', color_mode='rgb'\n",
        ")\n",
        "\n",
        "# 2. Define Model Function\n",
        "def create_model(final_activation):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        # Final layer with specified activation\n",
        "        layers.Dense(8, activation=final_activation)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 3. Configurations\n",
        "activation_configs = [\n",
        "    {'name': 'softmax', 'from_logits': False},\n",
        "    {'name': 'sigmoid', 'from_logits': False},\n",
        "    {'name': 'linear', 'from_logits': True}\n",
        "]\n",
        "\n",
        "results_final_act = {}\n",
        "\n",
        "# 4. Loop through configurations\n",
        "for config in activation_configs:\n",
        "    act_name = config['name']\n",
        "    from_logits = config['from_logits']\n",
        "    print(f\"\\n{'='*60}\\nProcessing model with final activation: {act_name}\\n{'='*60}\")\n",
        "\n",
        "    model_filename = os.path.join(models_dir, f'model_act_{act_name}.keras')\n",
        "    history_filename = os.path.join(models_dir, f'history_act_{act_name}.pkl')\n",
        "\n",
        "    if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "        print(f\"Loading existing model and history for {act_name}...\")\n",
        "        model = keras.models.load_model(model_filename)\n",
        "        with open(history_filename, 'rb') as f:\n",
        "            history = pickle.load(f)\n",
        "    else:\n",
        "        print(f\"Training {act_name} model...\")\n",
        "        model = create_model(act_name)\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=from_logits),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        history_obj = model.fit(\n",
        "            train_gen,\n",
        "            epochs=10,\n",
        "            validation_data=val_gen,\n",
        "            verbose=1\n",
        "        )\n",
        "        history = history_obj.history\n",
        "\n",
        "        # Save\n",
        "        model.save(model_filename)\n",
        "        with open(history_filename, 'wb') as f:\n",
        "            pickle.dump(history, f)\n",
        "        print(f\"Saved model and history for {act_name}.\")\n",
        "\n",
        "    # 5. Evaluate\n",
        "    print(f\"Evaluating {act_name} model on test data...\")\n",
        "    test_loss, test_acc = model.evaluate(test_gen, verbose=0)\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    results_final_act[act_name] = {'acc': test_acc, 'loss': test_loss, 'history': history}"
      ],
      "id": "ebf948a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh god, I just can't be friends with LLMs. Will it initialize generators every time now?"
      ],
      "metadata": {
        "id": "UC5EMo3eg0hG"
      },
      "id": "UC5EMo3eg0hG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0678ef8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Ensure results_final_act is available (from previous step)\n",
        "if 'results_final_act' in locals():\n",
        "    print(f\"{'='*60}\\nComparison of Final Layer Activations\\n{'='*60}\")\n",
        "\n",
        "    # 1. Bar Chart Comparison\n",
        "    labels = [name.capitalize() for name in results_final_act.keys()]\n",
        "    accuracies = [results_final_act[name]['acc'] for name in results_final_act.keys()]\n",
        "    losses = [results_final_act[name]['loss'] for name in results_final_act.keys()]\n",
        "\n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    rects1 = plt.bar(x - width/2, accuracies, width, label='Test Accuracy')\n",
        "    rects2 = plt.bar(x + width/2, losses, width, label='Test Loss')\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Test Metrics by Final Layer Activation')\n",
        "    plt.xticks(x, labels)\n",
        "    plt.legend()\n",
        "    plt.bar_label(rects1, padding=3, fmt='%.4f')\n",
        "    plt.bar_label(rects2, padding=3, fmt='%.4f')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Learning Curves Comparison\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for name in results_final_act:\n",
        "        acc = results_final_act[name]['history']['accuracy']\n",
        "        val_acc = results_final_act[name]['history']['val_accuracy']\n",
        "        plt.plot(acc, label=f'{name.capitalize()} Train')\n",
        "        plt.plot(val_acc, linestyle='--', label=f'{name.capitalize()} Val')\n",
        "    plt.title('Accuracy Comparison')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for name in results_final_act:\n",
        "        loss = results_final_act[name]['history']['loss']\n",
        "        val_loss = results_final_act[name]['history']['val_loss']\n",
        "        plt.plot(loss, label=f'{name.capitalize()} Train')\n",
        "        plt.plot(val_loss, linestyle='--', label=f'{name.capitalize()} Val')\n",
        "    plt.title('Loss Comparison')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Summary Text\n",
        "    print(\"\\nDetailed Metrics:\")\n",
        "    for name in results_final_act:\n",
        "        print(f\"{name.capitalize()}: Test Acc = {results_final_act[name]['acc']:.4f}, Loss = {results_final_act[name]['loss']:.4f}\")\n",
        "else:\n",
        "    print(\"Error: 'results_final_act' dictionary not found. Please run the training/evaluation step first.\")"
      ],
      "id": "e0678ef8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, it was actually around the same accuracy/loss for all three, which I didn't expect. Why was that?"
      ],
      "metadata": {
        "id": "J4IBkJYVhJ6b"
      },
      "id": "J4IBkJYVhJ6b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving test dataset as a csv file."
      ],
      "metadata": {
        "id": "gQR_RNcnhiBA"
      },
      "id": "gQR_RNcnhiBA"
    },
    {
      "cell_type": "code",
      "source": [
        "test.to_csv(models_dir + '/test.csv', index=False)"
      ],
      "metadata": {
        "id": "t3K9yaNlhUFF"
      },
      "id": "t3K9yaNlhUFF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh hell, I lost part of part 3 because I didn't commit my notebook and colab doesn't save file from github by default."
      ],
      "metadata": {
        "id": "ARQG6INEVJpu"
      },
      "id": "ARQG6INEVJpu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3. Backbone models and fine tuning"
      ],
      "metadata": {
        "id": "k82BffUGVl2R"
      },
      "id": "k82BffUGVl2R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split training set"
      ],
      "metadata": {
        "id": "MMb3X4j9Vrcx"
      },
      "id": "MMb3X4j9Vrcx"
    },
    {
      "cell_type": "code",
      "source": [
        "training_block_1 = train.sample(frac=0.5, random_state=42)\n",
        "training_block_2 = train.drop(training_block_1.index)\n",
        "\n",
        "print(f\"Training block 1 size: {len(training_block_1)}\")\n",
        "print(f\"Training block 2 size: {len(training_block_2)}\")"
      ],
      "metadata": {
        "id": "XAw04QKgVRSM"
      },
      "id": "XAw04QKgVRSM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There was a note about using LLMs, but I've lost it in unequal fight with Colab."
      ],
      "metadata": {
        "id": "rKfvRmd3Xgj6"
      },
      "id": "rKfvRmd3Xgj6"
    },
    {
      "cell_type": "code",
      "source": [
        "training_block_1.to_csv(models_dir + 'training_block_1.cvs')\n",
        "training_block_2.to_csv(models_dir + 'training_block_2.cvs')"
      ],
      "metadata": {
        "id": "ArFVB1xnX8Ya"
      },
      "id": "ArFVB1xnX8Ya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder Modelling: For Block 1 Images"
      ],
      "metadata": {
        "id": "nwycRDgak4PI"
      },
      "id": "nwycRDgak4PI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de318b6e"
      },
      "source": [
        "# Task\n",
        "Define a wrapper function `autoencoder_generator` that takes an existing `ImageDataGenerator` iterator and yields `(image, image)` pairs instead of `(image, label)` pairs, as autoencoders require the input to be the target. Then, using the existing `train`, `validate`, and `test` dataframes and the `datagen` (with `rescale=1./255`), create new base generators for RGB images (64x64) and wrap them to produce `train_gen_auto`, `val_gen_auto`, and `test_gen_auto`."
      ],
      "id": "de318b6e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76bc1c19"
      },
      "source": [
        "## Prepare Autoencoder Data Generators\n",
        "\n",
        "### Subtask:\n",
        "Define a generator wrapper and prepare data iterators for autoencoder training.\n"
      ],
      "id": "76bc1c19"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ebb6fa"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `autoencoder_generator` function to adapt data for autoencoder training (input equals output) and create the corresponding data generators using the train, validation, and test dataframes.\n",
        "\n"
      ],
      "id": "94ebb6fa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cca40960"
      },
      "source": [
        "# 1. Define generator wrapper for Autoencoder (Target = Input)\n",
        "def autoencoder_generator(generator):\n",
        "    while True:\n",
        "        # Extract batch. flow_from_dataframe yields (x, y)\n",
        "        # We disregard y (labels) and yield (x, x)\n",
        "        x, y = next(generator)\n",
        "        yield x, x\n",
        "\n",
        "# 2. Create Base Generators (using existing 'datagen')\n",
        "print(\"Setting up Base Generators for Autoencoder...\")\n",
        "train_gen_base = datagen.flow_from_dataframe(\n",
        "    dataframe=training_block_1,\n",
        "    x_col='file',\n",
        "    y_col='age_bins',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb'\n",
        ")\n",
        "\n",
        "val_gen_base = datagen.flow_from_dataframe(\n",
        "    dataframe=validate,\n",
        "    x_col='file',\n",
        "    y_col='age_bins',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb'\n",
        ")\n",
        "\n",
        "test_gen_base = datagen.flow_from_dataframe(\n",
        "    dataframe=test,\n",
        "    x_col='file',\n",
        "    y_col='age_bins',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb'\n",
        ")\n",
        "\n",
        "# 3. Create Autoencoder Iterators\n",
        "train_gen_auto = autoencoder_generator(train_gen_base)\n",
        "val_gen_auto = autoencoder_generator(val_gen_base)\n",
        "test_gen_auto = autoencoder_generator(test_gen_base)\n",
        "\n",
        "print(\"Autoencoder generators initialized.\")"
      ],
      "id": "cca40960",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3ec5bb3"
      },
      "source": [
        "## Define and Train Autoencoder\n",
        "\n",
        "### Subtask:\n",
        "Define the Convolutional Autoencoder architecture (encoder and decoder), compile it with MSE loss, and train it using the prepared data generators.\n"
      ],
      "id": "e3ec5bb3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6de2d54e"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the Convolutional Autoencoder architecture, compile it with MSE loss, and train it using the previously prepared autoencoder data generators.\n",
        "\n"
      ],
      "id": "6de2d54e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b409fb2e"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# 1. Define Autoencoder Architecture\n",
        "input_img = Input(shape=(64, 64, 3))\n",
        "\n",
        "# Encoder\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# Decoder\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "autoencoder.summary()\n",
        "\n",
        "# 2. Train the Model\n",
        "# Calculate steps per epoch manually since we are using generators derived from dataframes\n",
        "steps_per_epoch = len(train) // 32\n",
        "validation_steps = len(validate) // 32\n",
        "\n",
        "model_filename = os.path.join(models_dir, 'autoencoder.keras')\n",
        "history_filename = os.path.join(models_dir, 'autoencoder_history.pkl')\n",
        "\n",
        "if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "    print(\"Loading existing autoencoder model and history...\")\n",
        "    autoencoder = keras.models.load_model(model_filename)\n",
        "    with open(history_filename, 'rb') as f:\n",
        "        history_auto = pickle.load(f)\n",
        "else:\n",
        "    print(\"Training autoencoder...\")\n",
        "    history_obj = autoencoder.fit(\n",
        "        train_gen_auto,\n",
        "        epochs=15,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_gen_auto,\n",
        "        validation_steps=validation_steps,\n",
        "        verbose=1\n",
        "    )\n",
        "    history_auto = history_obj.history\n",
        "\n",
        "    # Save model and history\n",
        "    autoencoder.save(model_filename)\n",
        "    with open(history_filename, 'wb') as f:\n",
        "        pickle.dump(history_auto, f)\n",
        "    print(\"Saved autoencoder model and history.\")"
      ],
      "id": "b409fb2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "022b3986"
      },
      "source": [
        "# Task\n",
        "Visualize the autoencoder's performance by selecting a batch of test images from `test_gen_auto`, generating their reconstructions, and plotting them side-by-side. Then, add a discussion on why MSE was chosen as the loss function and what other metrics like MAE or SSIM could indicate. Finally, provide a summary of the autoencoder construction and an assessment of the image quality."
      ],
      "id": "022b3986"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa0e3c08"
      },
      "source": [
        "## Visualize Reconstruction\n",
        "\n",
        "### Subtask:\n",
        "Generate reconstructions for a batch of test images and visualize them alongside the originals.\n"
      ],
      "id": "aa0e3c08"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f1228aa"
      },
      "source": [
        "**Reasoning**:\n",
        "Retrieve a batch of test images, generate reconstructions using the trained autoencoder, and visualize the original vs. reconstructed images to assess performance.\n",
        "\n"
      ],
      "id": "9f1228aa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "630e949e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Retrieve a single batch of test images\n",
        "# The generator yields (x, x), so we take the first element\n",
        "x_test, _ = next(test_gen_auto)\n",
        "\n",
        "# 2. Generate reconstructed images\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "# 3. Visualize a subset of images\n",
        "n = 10  # Number of images to display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i])\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i])\n",
        "    plt.title(\"Reconstructed\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Autoencoder Reconstruction Results\")\n",
        "plt.show()"
      ],
      "id": "630e949e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the reconstructed image quality is somewhat worse than the original. I think that is a normal thing and attribute it to pooling and upsampling, where we could lose some information."
      ],
      "metadata": {
        "id": "ydkXtmvwhJ8Q"
      },
      "id": "ydkXtmvwhJ8Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5868fe3"
      },
      "source": [
        "### Discussion and Summary\n",
        "\n",
        "**Choice of Loss Function (MSE):**\n",
        "Mean Squared Error (MSE) is a standard loss function for autoencoders performing image reconstruction. It calculates the average squared difference between the pixel values of the original and reconstructed images. By penalizing larger errors more severely, MSE encourages the model to capture the general structure and color distribution effectively. However, it operates on a pixel-by-pixel basis, which can sometimes lead to blurry reconstructions as the model 'averages' out high-frequency details to minimize the overall error.\n",
        "\n",
        "**Alternative Metrics:**\n",
        "*   **MAE (Mean Absolute Error):** Calculates the average absolute difference. It is less sensitive to outliers than MSE and can sometimes result in slightly sharper edges, though it still doesn't explicitly capture structural information.\n",
        "*   **SSIM (Structural Similarity Index):** A perceptual metric that measures the similarity between two images based on luminance, contrast, and structure. Unlike pixel-wise metrics (MSE/MAE), SSIM correlates better with human perception of image quality. A high SSIM indicates that the reconstructed image preserves the structural information of the original well.\n",
        "\n",
        "**Autoencoder Construction Summary:**\n",
        "The model is a Convolutional Autoencoder designed for 64x64 RGB images.\n",
        "*   **Encoder:** Compresses the input into a lower-dimensional latent representation using a series of `Conv2D` layers (relu activation) followed by `MaxPooling2D` for downsampling.\n",
        "*   **Decoder:** Reconstructs the image from the latent representation using `Conv2D` layers and `UpSampling2D` to recover the spatial dimensions, ending with a `sigmoid` activation to output pixel values in the [0, 1] range.\n",
        "\n",
        "**Assessment of Image Quality:**\n",
        "Visually, the reconstructed images likely capture the global facial features, pose, and skin tone of the original images. However, fine details (like individual hair strands or skin texture) might be smoothed out or slightly blurry. This is typical for simple autoencoders trained with MSE, as the bottleneck forces the model to prioritize the most significant features required to approximate the input."
      ],
      "id": "d5868fe3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm curious to try SSIM loss function if I'll have time."
      ],
      "metadata": {
        "id": "CXtDLvPntoK4"
      },
      "id": "CXtDLvPntoK4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4fbc24c"
      },
      "source": [
        "# Task\n",
        "Use `training_block_2` to create a new `train_gen_transfer` data generator with the same settings as before (RGB, 64x64, sparse). Then, build a transfer learning model by extracting the encoder layers from the trained `autoencoder` (up to the bottleneck), followed by a `Flatten` layer, a `Dense` layer with 64 units (ReLU), and a final `Dense` output layer (Softmax). Freeze the encoder layers so their weights do not change during training. Train this new model for 10 epochs using `train_gen_transfer` and the existing `val_gen_base`. Finally, evaluate the model on `test_gen_base` and generate a bar chart comparing its accuracy and loss against the original RGB baseline model (available in `results_channels['rgb']`)."
      ],
      "id": "c4fbc24c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "508f8833"
      },
      "source": [
        "## Prepare Data Generators for Block 2\n",
        "\n",
        "### Subtask:\n",
        "Create a data generator for the second training block to be used for transfer learning.\n"
      ],
      "id": "508f8833"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8339b400"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the `train_gen_transfer` generator using the second training block dataframe and the existing datagen object.\n",
        "\n"
      ],
      "id": "8339b400"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bee3b387"
      },
      "source": [
        "print(\"Setting up Transfer Learning Generator (Block 2)...\")\n",
        "train_gen_transfer = datagen.flow_from_dataframe(\n",
        "    dataframe=training_block_2,\n",
        "    x_col='file',\n",
        "    y_col='age_bins',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb'\n",
        ")\n",
        "\n",
        "print(\"Transfer learning generator initialized.\")"
      ],
      "id": "bee3b387",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e748cf49"
      },
      "source": [
        "## Build Transfer Learning Model\n",
        "\n",
        "### Subtask:\n",
        "Construct the transfer learning model by extracting the encoder from the autoencoder, adding classification layers, and freezing the encoder weights.\n"
      ],
      "id": "e748cf49"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1993caa"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract the encoder part from the pre-trained autoencoder, freeze its layers, and add classification layers to create a transfer learning model.\n",
        "\n"
      ],
      "id": "e1993caa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be33ad5f"
      },
      "source": [
        "# 1. Extract Encoder\n",
        "# The bottleneck is at index 6 (MaxPooling2D after 3rd Conv block)\n",
        "encoder_output = autoencoder.layers[6].output\n",
        "encoder_model = keras.Model(inputs=autoencoder.input, outputs=encoder_output, name='encoder')\n",
        "\n",
        "# 2. Freeze Encoder\n",
        "encoder_model.trainable = False\n",
        "\n",
        "# 3. Build Transfer Learning Model\n",
        "model_transfer = models.Sequential([\n",
        "    encoder_model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(8, activation='softmax')\n",
        "], name='transfer_learning_model')\n",
        "\n",
        "# 4. Compile\n",
        "model_transfer.compile(optimizer='adam',\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "# 5. Summary\n",
        "model_transfer.summary()"
      ],
      "id": "be33ad5f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5415daac"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the transfer learning model (or load if available), evaluate it on the test set, and visualize the performance comparison against the baseline RGB model.\n",
        "\n"
      ],
      "id": "5415daac"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e196b62d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1. Train or Load Model\n",
        "model_filename = os.path.join(models_dir, 'model_transfer.keras')\n",
        "history_filename = os.path.join(models_dir, 'history_transfer.pkl')\n",
        "\n",
        "if os.path.exists(model_filename) and os.path.exists(history_filename):\n",
        "    print(\"Loading existing transfer learning model and history...\")\n",
        "    model_transfer = keras.models.load_model(model_filename)\n",
        "    with open(history_filename, 'rb') as f:\n",
        "        history_transfer = pickle.load(f)\n",
        "else:\n",
        "    print(\"Training transfer learning model...\")\n",
        "    history_obj = model_transfer.fit(\n",
        "        train_gen_transfer,\n",
        "        epochs=10,\n",
        "        validation_data=val_gen_base,\n",
        "        verbose=1\n",
        "    )\n",
        "    history_transfer = history_obj.history\n",
        "\n",
        "    # Save\n",
        "    model_transfer.save(model_filename)\n",
        "    with open(history_filename, 'wb') as f:\n",
        "        pickle.dump(history_transfer, f)\n",
        "    print(\"Saved transfer learning model and history.\")\n",
        "\n",
        "# 2. Evaluate on Test Data\n",
        "print(\"Evaluating transfer learning model on test data...\")\n",
        "test_loss_transfer, test_acc_transfer = model_transfer.evaluate(test_gen_base, verbose=0)\n",
        "print(f\"Transfer Model - Test Accuracy: {test_acc_transfer:.4f}\")\n",
        "print(f\"Transfer Model - Test Loss: {test_loss_transfer:.4f}\")\n",
        "\n",
        "# 3. Comparison Visualization\n",
        "print(f\"\\n{'='*60}\\nComparison: Transfer Learning vs Baseline RGB\\n{'='*60}\")\n",
        "\n",
        "# Retrieve Baseline Results\n",
        "# Assuming results_channels['rgb'] exists from previous steps\n",
        "if 'results_channels' in locals() and 'rgb' in results_channels:\n",
        "    baseline_acc = results_channels['rgb']['acc']\n",
        "    baseline_loss = results_channels['rgb']['loss']\n",
        "else:\n",
        "    # Fallback if variable lost, using hardcoded approximate values or re-evaluating would be needed,\n",
        "    # but assuming context is preserved.\n",
        "    print(\"Warning: Baseline results not found in memory. Using placeholders if necessary.\")\n",
        "    baseline_acc = 0.0\n",
        "    baseline_loss = 0.0\n",
        "\n",
        "labels = ['Baseline RGB', 'Transfer Learning']\n",
        "accuracies = [baseline_acc, test_acc_transfer]\n",
        "losses = [baseline_loss, test_loss_transfer]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "rects1 = plt.bar(x - width/2, accuracies, width, label='Test Accuracy')\n",
        "rects2 = plt.bar(x + width/2, losses, width, label='Test Loss')\n",
        "\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Test Metrics: Baseline vs Transfer Learning')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.bar_label(rects1, padding=3, fmt='%.4f')\n",
        "plt.bar_label(rects2, padding=3, fmt='%.4f')\n",
        "plt.show()\n",
        "\n",
        "# 4. Plot Learning Curves for Transfer Model\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_transfer['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_transfer['val_accuracy'], linestyle='--', label='Val Accuracy')\n",
        "plt.title('Transfer Learning Model - Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_transfer['loss'], label='Train Loss')\n",
        "plt.plot(history_transfer['val_loss'], linestyle='--', label='Val Loss')\n",
        "plt.title('Transfer Learning Model - Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e196b62d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Haha, it didn't work out the way I thought it will. I mean, it's around the same model? We've taken only encoder, so we didn't attempt to reconstruct the image, so we didn't lose some information on it. I wonder why results for transfer learning a worse then? Only because there were less examples to train on (block 2 only)?"
      ],
      "metadata": {
        "id": "X9zPX6-bq69T"
      },
      "id": "X9zPX6-bq69T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asking Gemini.\n"
      ],
      "metadata": {
        "id": "VdpgZxgFsO8S"
      },
      "id": "VdpgZxgFsO8S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It says that additionaly we've compiled autoencoder with 'MSE' loss function as opposed to 'sparse_categorical_crossentropy' for the original model. That seems like it might bu true, right."
      ],
      "metadata": {
        "id": "8103RbVytKch"
      },
      "id": "8103RbVytKch"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backbone Model Re-Use"
      ],
      "metadata": {
        "id": "WNuplVk9test"
      },
      "id": "WNuplVk9test"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2sONl6Ldr0J8"
      },
      "id": "2sONl6Ldr0J8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}